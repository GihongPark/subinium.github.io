---
title : \[Keras Study\] 8장. 생성 모델을위한 딥러닝
category :
  - ML
tag :
  - keras
  - deep-learning
  - AI
  - machine learning
  - colab
sidebar:
  nav: sidebar-keras

use_math : true

header:
  teaser : /assets/images/category/ml.jpg
  overlay_image : /assets/images/category/ml.jpg

published: false
---

신기한 딥러닝의 세계...

본 문서는 [케라스 창시자에게 배우는 딥러닝] 책을 기반으로 하고 있으며, subinium(본인)이 정리하고 추가한 내용입니다. 생략된 부분과 추가된 부분이 있으니 추가/수정하면 좋을 것 같은 부분은 댓글로 이야기해주시면 감사하겠습니다.

AI는 더 이상 소극적 작업이 아닌 **확장된 지능(augmented intelligence)** 의 역할을 하고 있습니다.
그림, 음악 등 다양한 분야에서 사람의 능력을 증가시킬 수 있는 것입니다. 딥러닝은 통계적 구조를 학습하며 이미지, 음악, 글 등의 **잠재 공간(latent space)** 를 학습합니다.

이 장에서는 예술 창작에 딥러닝이 사용되는 예시를 알아보도록 합니다. 기술과 예술의 조화를 보도록 합시다.

## 8.1 LSTM으로 텍스트 생성하기

이 절에서는 순환 신경망을 이용한 시퀀스 데이터 생성을 목표로 합니다.
책의 예시는 텍스트를 다루지만, 음악은 음표를 그림은 붓질을 적용할 수 있습니다.
예술만이 아닌 음성 합성과 챗봇의 대화 기능에도 적용하고 있습니다.

### 8.1.1 생성 RNN의 간단한 역사

***생략***

### 8.1.2 시퀀스 데이터를 어떻게 생성할까?

딥러닝에서 시퀀스 데이터를 생성하는 일반적인 방법은 이전 토큰을 입력으로 사용해서 시퀀스의 다음 1개 또는 몇 개의 토큰을 예측하는 것입니다.(RNN이나  컨브넷으로)
예를 들어 "the cat is on ma"란 입력이 주어지면 다음 글자인 타깃 "t"를 예측하도록 네트워크를 훈련합니다.
텍스트를 다룰 때 토큰은 보통 단어 또는 글자입니다. 이전 토큰들이 주어졌을 때 다음 토큰의 확률을 모델링할 수 있는 네트워크 를 **언어 모델(language model)** 이라고 부릅니다. 언어의 통계적 구조인 잠재 공간을 탐색합니다.

언어 모델을 훈련하고 나면 새로운 시퀀스를 생성할 수 있습니다. 초기 텍스트 문자열을 주입하고(**조건 데이터(conditioning data)**) 새로운 글자난 단어를 생성합니다. 생성된 출력은 다시 입력 데이터로 추가됩니다.
반복을 통해 모델이 훈련한 데이터 구조가 반영된 임의의 길이를 가진 시퀀스를 생성할 수 있습니다.

이 절에서는 LSTM 층을 사용합니다. 텍스트 말뭉치에서 N개의 글자로 이루어진 문자열을 추출하여 주입하고 N+1 번째 글자를 예측하도록 훈련합니다. 모델의 출력은 가능한 모든 글자에 해당하는 소프트맥스 값입니다.
이 LSTM을 **글자 수준의 신경망 언어 모델(character-level neural language model)** 이라고 부릅니다.

### 8.1.3 샘플링 전략의 중요성

텍스트를 생성할 때 다음 글자를 선택하는 방법은 매우 중요합니다. Naive한 방법으로 가장 높은 확률을 가진 글자를 선택하는 **탐욕적 샘플링(greedy sampling)** 이 있습니다. 이 방법은 반복적이고 예상 가능한 문자열을 만들기 때문에 논리적인 언어처럼 보이지 않습니다.

여기서 더 나아가 무작위성을 추가하여 **확률적 샘플링(stochastic sampling)** 을 할 수 있습니다. 소프트맥스 결과값에서 가중치에 따라 랜덤으로 선택하는 것입니다. 이와 같이하면 글이 새로운 방향으로 흘러갈 확률도 많습니다. 하지만 여기서는 무작위성의 양을 조절할 수 없다는 단점이 있습니다.

샘플링 과정에서 확률적인 양을 조절하기 위해 **소프트맥스 온도(softmax temperature)** 라는 파라미터를 사용합니다.
이 파라미터는 샘플링에 사용되는 확률 분포의 엔트로피를 나타냅니다. 얼마나 놀라운 또는 예상되는 글자를 선택할지 결정합니다.

`temperature` 값이 주어지면 다음과 같이 가중치를 적용하여 원본 확률 분포에서 새로운 확률 분포를 계산합니다.
높은 온도는 엔트로피가 높은 샘플링 분포를 만들어 더 생소한 데이터를 생성하고, 낮은 온도는 예상할 수 있는 데이터를 생성합니다.

### 8.1.4 글자 수준의 LSTM 텍스트 생성 모델 구현

이 절에서는 위 아이디어를 케라스로 구현해봅니다.
언어 모델을 학습하기 위해 많은 텍스트 데이터가 필요합니다. *위키피디아* 나 *반지의 제왕* 같은 아주 큰 텍스트 파일이나 묶음을 이용할 수 있습니다. 책에서는 19세기 후반의 독일 철학자 니체의 글(영문)을 이용합니다. 학습할 언어 모델은 일반적인 영어 모델이 아니라 니체의 문체와 특정 주체를 따르는 모델일 것입니다.

#### 데이터 전처리

먼저 말뭉치를 내려받아 소문자로 바꿉니다.

``` python
# 코드 8-2 원본 텍스트 파일을 내려받아 파싱하기
import keras
import numpy as np

path = keras.utils.get_file('nietzsche.txt', origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')
text = open(path).read().lower()
print('length  of corpus : ', len(text))
```

```
Using TensorFlow backend.
Downloading data from https://s3.amazonaws.com/text-datasets/nietzsche.txt
606208/600901 [==============================] - 0s 0us/step
length  of corpus :  600893
```

그 다음 maxlen 길이를 가진 시퀀스르 중복하여 추출합니다. 추출된 시퀀스를 원-핫 인코딩으로 변환하고 `(sequences, maxlen, unique_characters)`인 3D 넘파이 배열 x로 합칩니다. 동시에 훈련 샘플에 상응하는 타깃을 담은 배열 y를 준비합니다. 타깃은 추출된 시퀀스 다음에 오는 원-핫 인코딩된 글자입니다.

``` python
# 코드 8-3 글자 시퀀스 벡터화하기

maxlen = 60
step = 3

sentences = []
next_chars = []

for i in range(0, len(text) - maxlen, step):
  sentences.append(text[i:i+maxlen])
  next_chars.append(text[i+maxlen])

print('Number of sequences : ', len(sentences))

chars = sorted(list(set(text)))
print('고유한 글자: ', len(chars))
char_indices = dict((char, chars.index(char)) for char in chars)

print('Vectorize...')

x = np.zeros((len(sentences),maxlen, len(chars)), dtype=np.bool)
y = np.zeros((len(sentences), len(chars)), dtype=np.bool)
for i, sentence in enumerate(sentences):
  for t, char in enumerate(sentence):
    x[i, t, char_indices[char]] = 1
  y[i, char_indices[next_chars[i]]] = 1
```

```
Number of sequences :  200278
고유한 글자:  57
Vectorize...
```

#### 네트워크 구성

이 네트워크는 하나의 LSTM 층과 그 뒤에 Dense 분류기가 뒤따릅니다. 분류기는 가능한 모든 글자에 대한 소프트맥스 출력을 만듭니다. 순환 신경망이 시퀀스 데이터를 생성하는 유일한 방법은 아닙니다. 최근에는 1D 컨브넷도 이런 작업에 아주 잘 들어맞는다는 것이 밝혀졌습니다. (6장)

``` python
# 코드 8-4 다음 글자를 예측하기 위한 단일 LSTM 모델
from keras import layers

model = keras.models.Sequential()
model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))
model.add(layers.Dense(len(chars), activation='softmax'))
```

타깃이 원-핫 인코딩되어 있기 때문에 모델을 훈련하기 위해 `categorical_crossentropy` 손실을 사용합니다.

``` python
# 코드 8-5 모델 컴파일 설정하기
optimizer = keras.optimizers.RMSprop(lr=0.01)
model.compile(loss='categorical_crossentropy', optimizer=optimizer)
```

#### 언어 모델 훈련과 샘플링

훈련된 모델과 시드(seed)로 쓰일 간단한 텍스트가 주어지면 다음과 같이 반복하여 새로운 텍스트를 생성할 수 있습니다.

1. 지금까지 생성된 텍스트를 주입하여 모델에서 다음 글자에 대한 확률 분포를 뽑습니다.
2. 특정 온도로 이 확률 분포의 가중치를 조정합니다.
3. 가중치가 조정된 분포에서 무작위로 새로운 글자를 샘플링합니다.
4. 새로운 글자를 생성된 텍스트의 끝에 추가합니다.

다음 코드는 모델에서 나온 원본 확률 분포의 가중치를 조정하고 새로운 글자의 인덱스를 추출합니다.

``` python
# 코드 8-6 모델의 예측이 주어졌을 떄 새로운 글자를 샘플링하는 함수
def sample(preds, temperature=1.0):
  preds = np.asarray(preds).astype('float64')
  preds = np.log(preds) / temperature
  exp_preds = np.exp(preds)
  preds = exp_preds / np.sum(exp_preds)
  probas = np.random.multinomial(1, preds, 1)
  return np.argmax(probas)
```

마지막으로 반복적으로 훈련하고 텍스트를 생성하는 반복문입니다. 에포크마다 학습이 끝난 후 여러 가지 온도를 사용하여 텍스트를 사용합니다. 이렇게 하면 모델이 수렴하면서 생성된 텍스트를 어떻게 진화하는지 볼 수 있습니다. 온도가 샘플링 전략에 미치는 영향도 보여줍니다.

``` python
# 코드 8-7 텍스트 생성 루프

```

### 8.1.5 정리

## 8.2 딥드림

### 8.2.1 케라스 딥드림 구현

### 8.2.2 정리

## 8.3 뉴럴 스타일 트랜스퍼

### 8.3.1 콘텐츠 손실

### 8.3.2 스타일 손실

### 8.3.3 케라스에서 뉴럴 스타일 트랜스퍼 구현하기

### 8.3.4 정리

## 8.4 변이형 오토인코드를 사용한 이미지 생성

### 8.4.1 이미지의 잠재 공간에서 샘플링하기

### 8.4.2 이미지 변형을 위한 개념 벡터

### 8.4.3 변이형 오토인코더

### 8.4.4 정리

## 8.5 적대적 생성 신경망 소개

### 8.5.1 GAN 구현 방법

### 8.5.2 훈련 방법

### 8.5.3 생성자

### 8.5.4 판별자

### 8.5.5 적대적 네트워크

### 8.5.6 DCGAN 훈련 방법

### 8.5.7 정리

## 8.6 요약
