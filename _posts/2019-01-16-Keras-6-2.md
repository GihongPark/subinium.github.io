---
title : \[Keras Study\] 6장. 텍스트와 시퀀스를 위한 딥러닝 (2)
category :
  - ML
tag :
  - keras
  - deep-learning
  - AI
  - machine learning
  - colab
sidebar:
  nav: sidebar-keras

use_math : true
header:
  teaser : /assets/images/category/ml.jpg
  overlay_image : /assets/images/category/ml.jpg
published : false
---

텍스트 데이터 + 순환 신경망 + 시퀀스 데이터의 처리

본 문서는 [케라스 창시자에게 배우는 딥러닝] 책을 기반으로 하고 있으며, subinium(본인)이 정리하고 추가한 내용입니다. 생략된 부분과 추가된 부분이 있으니 추가/수정하면 좋을 것 같은 부분은 댓글로 이야기해주시면 감사하겠습니다.

- 본 정리는 [6장. 텍스트와 시퀀스를 위한 딥러닝 (1)](/Keras-6-1)과 연결된 내용입니다.
- 예제들은 Colab 에서 진행했습니다.
- Colab에서 진행하는데 돌리면 멈춰서, 이번에도 중반까지 시도했습니다.
- 기본적으로 뒤의 예제는 1시간 이상 필요합니다.

## 6.3 순환 신경망의 고급 사용법

순환 신경망의 성능과 일반화 능력을 향상시키기 위한 세 가지 고급 기술을 살펴보겠습니다.
온도 예측 문제와 함께 각 개념을 이해해보도록 하겠습니다. 이 문제는 다음과 같은 특성을 알 수 있습니다.

- 시계열 데이터
- 건물 옥상에 설치된 센서에서 취득한 온도, 기압, 습도 데이터
- 마지막 데이터 포인트에서 24시간 이후의 온도를 예측
- 전형적이고 도전적인 문제

기법들은 다음과 같습니다.

- **순환 드롭아웃(recurrent dropout)** : 순환 층에서 과대적합을 방지하기 위해 케라스에 내장되어 있는 드롭아웃을 사용
- **스태킹 순환 층(stacking recurrent layer)** : 네트워크의 표현 능력(representation power)을 증가 - 대신 계산 비용이 많이듬
- **양방향 순환 층(bidirectional recurrent layer)** : 순환 네트워크에 같은 정보를 다른 방향으로 주입하여 정확도를 높이고 기억을 좀 더 오래 유지시킵니다.

### 6.3.1 기온 예측 문제

이 절에서 사용하는 예제는 날씨 시계열 데이터샛을 사용합니다. 이 데이터는 독일 예나시에 있는 막스 플랑크 생물지구화학 연구소의 지상 관측소에서 수집한 것입니다.

- 수년간의 데이터
- 10분 단위의 데이터
- 14개의 관측치(특징)

본 문제에서는 2009~2016년 사이의 데이터만 사용합니다.
데이터는 다음과 같이 내려받을 수 있습니다.

``` shell
wget https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip
unzip jena_climate_2009_2016.csv.zip
```

다운을 받고 데이터를 살펴봅시다.

``` python
# 코드 6-28 예나의 날씨 데이터셋 조사하기

import os

data_dir = './jena_climate'
fname = os.path.join(data_dir, 'jena_climate_2009_2016.csv')

f = open(fname)
data = f.read()
f.close()

lines = data.split('\n')
header = lines[0].split(',')
lines = lines[1:]

print(header)
print(len(lines))
```

```
['"Date Time"', '"p (mbar)"', '"T (degC)"', '"Tpot (K)"', '"Tdew (degC)"', '"rh (%)"', '"VPmax (mbar)"', '"VPact (mbar)"', '"VPdef (mbar)"', '"sh (g/kg)"', '"H2OC (mmol/mol)"', '"rho (g/m**3)"', '"wv (m/s)"', '"max. wv (m/s)"', '"wd (deg)"']

420551
```

총 42만 551개의 데이터가 있습니다 420만분 정도의 데이터입니다.
이제 이를 넘파이 배열로 바꿉니다.

```python
# 코드 6-29 데이터 파싱하기
import numpy as np

float_data = np.zeros((len(lines), len(header)-1))
for i, line in enumerate(lines):
  values = [float(x) for x in line.split(',')[1:]]
  float_data[i,:] = values
```

온도의 경향성을 보기위해 가볍게 그래프를 그려봅니다.

```python
# 코드 6-30 식ㅖ열 데이터 온도 그래프 그리기
from matplotlib import pyplot as plt

temp = float_data[:, 1]
plt.plot(range(len(temp)), temp)
```

![온도데이터](https://i.imgur.com/bsJPnfG.png)

최고 기온과 최저 기온을 보니 저기도 헬인건 분명한 것을 알 수 있습니다!
이건 중요한게 아니고 처음 10일간의 온도 데이터를 따로 보면 다음과 같습니다. 24*60 / 10 * 10 = 1440개의 데이터 포인트로 그린 그래프입니다. 책에선 10일치만 했으니 전 20일치를 보겠습니다. 이유는 없습니다.

``` python
# 코드 6-31 처음 20일간 온도 그래프 그리기
plt.plot(range(144*20), temp[:144*20])
```

![온도데이터2](https://i.imgur.com/vrm4GJV.png)

데이터를 관찰하면 매우 추운 날이 점점 따뜻해지는 것을 알 수 있습니다.
한파가 왔다가 점점 날이 풀리는 것을 알 수 있습니다.

분명 하루의 주기성이 있으며 몇 달 그리고 몇 년간의 데이터가 있으니 주기를 통해 예측을 할 수 있을 것 같습니다.

### 6.3.2 데이터 준비

이 문제의 정확한 정의는 다음과 같습니다.

lookback 타임스텝만큼 이전으로 돌아가서 매 steps 타임스텝마다 샘플링합니다. 이 데이터를 바탕으로 delay 타임스텝 이후 온도를 예측할 수 있을까요?

- lookback = 1440: 10일 전 데이터로 돌아갑니다.
- steps = 6: 1시간마다 데이터 포인트 하나를 샘플링합니다.
- delay = 144: 24시간이 지난 데이터가 타깃이 됩니다.

시작하기 전에 두가지 작업을 처리해야 합니다.

- 신경망에 주입할 수 있는 형태로 데이터를 전처리합니다. 데이터가 이미 수치형이므로 추가적인 벡터화가 필요하지 않습니다., 하지만 데이터에 있는 각 시계열 특성은 범위가 서로 다르므로 정규화해야합니다.
- float_data 배열을 받아 과거 데이터의 배치와 미래 타깃 온도를 추출하는 파이썬 제너레이터를 만듭니다. 이 데이터셋에 있는 샘플은 중복이 많습니다. 모든 샘플을 각기 메모리에 적재하는 것은 낭비가 심하므로 원본 데이터를 사용하여 그때그때 배치를 만들어야합니다.

처음 20만 개의 타임스펩을 훈련 데이터로 사용할 것이니 이로 정규화를 진행합니다.

``` python
# 코드 6-32 데이터 정규화하기
mean = float_data[:200000].mean(axis=0)
float_data -= mean
std = float_data[:200000].std(axis=0)
float_data /= std
```

아래 코드는 여기서 사용할 제너레이터입니다. 이 제너레이터 함수는 `(samples, targets)` 튜플을 반복적으로 반환합니다. 각각은 입력 데이터로 사용할 배치와 이에 대응되는 타깃의 온도의 배열입니다. 다음과 같은 매개변수를 가집니다.

- `data` : 정규화를 진행한 부동 소수 데이터로 이루어진 원본 배열
- `lookback` : 입력으로 사용하기 위해 거슬러 올라갈 타임스텝
- `delay` : 타깃으로 사용할 미래의 타임스텝
- `min_index와 max_index` : 추출할 타임스텝의 범위를 지정하기 위한 data 배열의 인덱스, 검증 데이터와 테스트 데이터를 분리하는 데 사용
- `shuffle` : 샘플을 섞을지, 시간 순서대로 추출할지 결정
- `batch_size` : 배치의 샘플 수
- `step` : 테이터를 샘플링할 타임스텝 간격. 1시간에 하나의 데이터 포인트를 추출하기 위해 6으로 지정

``` python
# 코드 6-33 시계열 데이터와 타깃을 반환하는 제너레이터 함수

def generator(data, lookback, delay, min_index, max_index,
              shuffle=False, batch_size=128, step=6):
    if max_index is None:
        max_index = len(data) - delay - 1
    i = min_index + lookback
    while 1:
        if shuffle:
            rows = np.random.randint(
                min_index + lookback, max_index, size=batch_size)
        else:
            if i + batch_size >= max_index:
                i = min_index + lookback
            rows = np.arange(i, min(i + batch_size, max_index))
            i += len(rows)

        samples = np.zeros((len(rows),
                           lookback // step,
                           data.shape[-1]))
        targets = np.zeros((len(rows),))
        for j, row in enumerate(rows):
            indices = range(rows[j] - lookback, rows[j], step)
            samples[j] = data[indices]
            targets[j] = data[rows[j] + delay][1]
        yield samples, targets
```

이제 generator로 훈련용, 검증용, 테스트용으로 3개의 제너레이터를 만듭니다.
각 제너레이터는 원본 데이터에서 다른 시간대를 사용합니다. 훈련 제너레이터는 처음 20만 개 타임스텝을 사용하고, 검증 제너레이터는 그다음 10만 개를 사용하고, 테스트 제너레이터는 나머지를 사용합니다.

``` python
# 코드 6-34 후련, 검증, 테스트 제너레이터 준비하기
lookback = 1440
step = 6
delay = 144
batch_size = 128

train_gen = generator(float_data,
                      lookback=lookback,
                      delay=delay,
                      min_index=0,
                      max_index=200000,
                      shuffle=True,
                      step=step,
                      batch_size=batch_size)
val_gen = generator(float_data,
                    lookback=lookback,
                    delay=delay,
                    min_index=200001,
                    max_index=300000,
                    step=step,
                    batch_size=batch_size)
test_gen = generator(float_data,
                     lookback=lookback,
                     delay=delay,
                     min_index=300001,
                     max_index=None,
                     step=step,
                     batch_size=batch_size)

# 전체 검증 세트를 순회하기 위해 val_gen에서 추출할 횟수
val_steps = (300000 - 200001 - lookback) // batch_size

# 전체 테스트 세트를 순회하기 위해 test_gen에서 추출할 횟수
test_steps = (len(float_data) - 300001 - lookback) // batch_size
```

### 6.3.3 상식 수준의 기준점

``` python
# 코드 6-35 상식적인 기준 모델의 MAE 계산하기

def evaluate_naive_method():
    batch_maes = []
    for step in range(val_steps):
        samples, targets = next(val_gen)
        preds = samples[:, -1, 1]
        mae = np.mean(np.abs(preds - targets))
        batch_maes.append(mae)
    print(np.mean(batch_maes))

evaluate_naive_method()
```

``` python
# 코드 6-36 MAE 섭씨 단위로 변환하기

0.29 * std[1]
```

### 6.3.4 기본적인 머신 러닝 방법

``` python
# 코드 6-37 완전 연결 모델을 훈련하고 평가하기

from keras.models import Sequential
from keras import layers
from keras.optimizers import RMSprop

model = Sequential()
model.add(layers.Flatten(input_shape=(lookback // step, float_data.shape[-1])))
model.add(layers.Dense(32, activation='relu'))
model.add(layers.Dense(1))

model.compile(optimizer=RMSprop(), loss='mae')
history = model.fit_generator(train_gen,
                              steps_per_epoch=500,
                              epochs=20,
                              validation_data=val_gen,
                              validation_steps=val_steps)
```

``` python
# 코드 6-38 결과 그래프 그리기
import matplotlib.pyplot as plt

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(loss) + 1)

plt.figure()

plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()
```

### 6.3.5 첫 번째 순환 신경망

``` python
# 코드 6-39 GRU를 사용한 모델을 훈련하고 평가하기
from keras.models import Sequential
from keras import layers
from keras.optimizers import RMSprop

model = Sequential()
model.add(layers.GRU(32, input_shape=(None, float_data.shape[-1])))
model.add(layers.Dense(1))

model.compile(optimizer=RMSprop(), loss='mae')
history = model.fit_generator(train_gen,
                              steps_per_epoch=500,
                              epochs=20,
                              validation_data=val_gen,
                              validation_steps=val_steps)
```

### 6.3.6 과대적합을 감소하기 위해 순환 드롭아웃 사용하기

``` python
# 코드 6-40 드롭아웃 규제된 GRU를 사용한 모델을 훈련하고 평가하기
from keras.models import Sequential
from keras import layers
from keras.optimizers import RMSprop

model = Sequential()
model.add(layers.GRU(32,
                     dropout=0.2,
                     recurrent_dropout=0.2,
                     input_shape=(None, float_data.shape[-1])))
model.add(layers.Dense(1))

model.compile(optimizer=RMSprop(), loss='mae')
history = model.fit_generator(train_gen,
                              steps_per_epoch=500,
                              epochs=40,
                              validation_data=val_gen,
                              validation_steps=val_steps)
```

### 6.3.7 스태킹 순환 층

``` python
# 코드 6-41 드롭아웃으로 규제하고 스태킹한 GRU 모델을 훈련하고 평가하기
from keras.models import Sequential
from keras import layers
from keras.optimizers import RMSprop

model = Sequential()
model.add(layers.GRU(32,
                     dropout=0.1,
                     recurrent_dropout=0.5,
                     return_sequences=True,
                     input_shape=(None, float_data.shape[-1])))
model.add(layers.GRU(64, activation='relu',
                     dropout=0.1,
                     recurrent_dropout=0.5))
model.add(layers.Dense(1))

model.compile(optimizer=RMSprop(), loss='mae')
history = model.fit_generator(train_gen,
                              steps_per_epoch=500,
                              epochs=40,
                              validation_data=val_gen,
                              validation_steps=val_steps)
```

### 6.3.8 양방향 RNN 사용하기

``` python
# 코드 6-42 거꾸로 된 시퀀스를 사용한 LSTM을 훈련하고 평가하기
def reverse_order_generator(data, lookback, delay, min_index, max_index,
                            shuffle=False, batch_size=128, step=6):
    if max_index is None:
        max_index = len(data) - delay - 1
    i = min_index + lookback
    while 1:
        if shuffle:
            rows = np.random.randint(
                min_index + lookback, max_index, size=batch_size)
        else:
            if i + batch_size >= max_index:
                i = min_index + lookback
            rows = np.arange(i, min(i + batch_size, max_index))
            i += len(rows)

        samples = np.zeros((len(rows),
                           lookback // step,
                           data.shape[-1]))
        targets = np.zeros((len(rows),))
        for j, row in enumerate(rows):
            indices = range(rows[j] - lookback, rows[j], step)
            samples[j] = data[indices]
            targets[j] = data[rows[j] + delay][1]
        yield samples[:, ::-1, :], targets

train_gen_reverse = reverse_order_generator(
    float_data,
    lookback=lookback,
    delay=delay,
    min_index=0,
    max_index=200000,
    shuffle=True,
    step=step,
    batch_size=batch_size)
val_gen_reverse = reverse_order_generator(
    float_data,
    lookback=lookback,
    delay=delay,
    min_index=200001,
    max_index=300000,
    step=step,
    batch_size=batch_size)

model = Sequential()
model.add(layers.GRU(32, input_shape=(None, float_data.shape[-1])))
model.add(layers.Dense(1))

model.compile(optimizer=RMSprop(), loss='mae')
history = model.fit_generator(train_gen_reverse,
                              steps_per_epoch=500,
                              epochs=20,
                              validation_data=val_gen_reverse,                               validation_steps=val_steps)
```

``` python
# 코드 6-43 양방향 LSTM을 훈련하고 평가하기
model = Sequential()
model.add(layers.Embedding(max_features, 32))
model.add(layers.Bidirectional(layers.LSTM(32)))
model.add(layers.Dense(1, activation='sigmoid'))

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
history = model.fit(x_train, y_train, epochs=10, batch_size=128, validation_split=0.2)

```

``` python
# 코드 6-44 양방향 GRU 훈련하기
from keras.models import Sequential
from keras import layers
from keras.optimizers import RMSprop

model = Sequential()
model.add(layers.Bidirectional(
    layers.GRU(32), input_shape=(None, float_data.shape[-1])))
model.add(layers.Dense(1))

model.compile(optimizer=RMSprop(), loss='mae')
history = model.fit_generator(train_gen,
                              steps_per_epoch=500,
                              epochs=40,
                              validation_data=val_gen,
                              validation_steps=val_steps)
```

### 6.3.9 더 나아가서

### 6.3.10 정리

## 6.4 컨브넷을 사용한 시퀀스 처리

### 6.4.1 시퀀스 데이터를 위한 1D 합성곱 이해하기

### 6.4.2 시퀀스 데이터를 위한 1D 풀링

### 6.4.3 1D 컨브넷 구현

### 6.4.4 CNN과 RNN을 연결하여 긴 시퀀스 처리하기

### 6.4.5 정리

## 6.5 요약
