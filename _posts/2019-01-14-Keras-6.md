---
title : \[Keras Study\] 6장. 텍스트와 시퀀스를 위한 딥러닝
category :
  - ML
tag :
  - keras
  - deep-learning
  - AI
  - machine learning
  - colab
sidebar:
  nav: sidebar-keras

use_math : true
header:
  teaser : /assets/images/category/ml.jpg
  overlay_image : /assets/images/category/ml.jpg
---

텍스트 데이터 + 순환 신경망 + 시퀀스 데이터의 처리

본 문서는 [케라스 창시자에게 배우는 딥러닝] 책을 기반으로 하고 있으며, subinium(본인)이 정리하고 추가한 내용입니다. 생략된 부분과 추가된 부분이 있으니 추가/수정하면 좋을 것 같은 부분은 댓글로 이야기해주시면 감사하겠습니다.

이번 장에는 텍스트(단어의 시퀀스 또는 문자의 시퀀스), 시계열 또는 일반적인 시퀀스 데이터를 처리할 수 있는 딥러닝 모델을 살펴봅니다. **순환 신경망(recurrent neural network)과 1D 컨브넷(1D convnet)** 두가지입니다.

다음 문제에서 이런 알고리즘을 사용합니다.

- 문서 분류나 시계열 분류, 예를 들어 글의 주제나 책의 저자 식별하기
- 시계열 비교, 예를 들어 두 문서나 두 주식 가격이 얼마나 밀접하게 관련이 있는지 추정하기
- 시퀀스-투-시퀀스 학습. 예를 들어 영어 문장을 프랑스어로 변환하기
- 감성 분석. 예를 들어 트윗이나 영화 리뷰가 긍정적인지 부정적인지 분류하기
- 시계열 예측. 예를 들어 어떤 지역의 최근 날씨 데이터가 주어졌을 때 향후 날씨 예측하기

2가지 문제를 집중하여 다룹니다. IMDB 데이터셋의 감정 분석과 기온 예측입니다.

## 6.1 텍스트 데이터 다루기

텍스트는 가장 흔한 시퀀스의 형태입니다. 보통은 단어 수준으로 작업하는 경우가 많습니다.
이 장에서는 기초적인 자연어 이해 문제를 처리할 수 있습니다.

- 문서 분류
- 감정 분석
- 저자 식별
- 제한된 질문 응답

텍스트를 이해하는 것이 아닌 통계적 구조를 통해 문제를 해결합니다. 컴퓨터 비전과 같이 패턴 인식 중 하나입니다.

모든 신경망과 마찬가지로 전처리를 통해 입력할 수 있는 데이터 형태로 만들어야 합니다. 이렇게 텍스트를 수치형 텐서로 변환하는 과정을 **텍스트 벡터화(vectorizing text)** 라고 부릅니다. 이는 다음과 같은 방법들이 있습니다.

- 텍스트를 단어로 나누고 각 단어를 하나의 벡터로 변환합니다.
- 텍스트를 문자로 나누고 각 문자를 하나의 벡터로 변환합니다.
- 텍스트에서 단어나 문자의 **n-gram** 을 추출하여 각 n-그램을 하나의 벡터로  변환합니다. n-gram은 연속된 단어나 문자의 그룹으로 텍스트에서 단어나 문자를 하나씩 이동하면서 추출합니다.

텍스트를 나누는 이런 단위를 **토큰(token)** 이라고 합니다. 그리고 텍스트를 토큰으로 나누는 것을 **토큰화(tokenization)** 라고 합니다. 텍스트 데이터는 이런 토큰화를 적용하고, 수치형 벡터에 연결하는 작업이 필요합니다. 여기서 2가지 방법을 소개합니다.

1. **원-핫 인코딩(one-hot encoding)**
2. **토큰 임베딩(token embedding)** 또는 **단어 임베딩(word embedding)**

### 6.1.1 단어와 문자의 원-핫 인코딩

원-핫 인코딩은 이미 3장에서 IMDB에서 사용해본 방법입니다.
단어 별로 인덱스를 부여하고 이 정수 인덱스 i를 크기가 N(어휘 사전 크기)인 이진 벡터로 변환합니다. i번째 원소만 1이고 나머지는 모두 0입니다.

케라스에는 원본 텍스트 데이터를 단어 또는 문자 수준의 원-핫 인코딩으로 변환해주는 유틸리티가 있습니다. 특수 문자를 제거하거나 빈도가 높은 단어만 사용하는 등 기능이 있습니다.

```python
# 코드 6-3 케라스를 사용한 단어 수준의 원-핫 인코딩하기

from keras.preprocessing.text import Tokenizer

samples = ['The cat sat on the mat.', 'The dog ate my homework.']

# 가장 빈도가 높은 1,000개의 단어만 선택하도록 Tokenizer 객체를 만듭니다.
tokenizer = Tokenizer(num_words=1000)
# 단어 인덱스를 구축합니다.
tokenizer.fit_on_texts(samples)

# 문자열을 정수 인덱스의 리스트로 변환합니다.
sequences = tokenizer.texts_to_sequences(samples)

# 직접 원-핫 이진 벡터 표현을 얻을 수 있습니다.
# 원-핫 인코딩 외에 다른 벡터화 방법들도 제공합니다!
one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')

# 계산된 단어 인덱스를 구합니다.
word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))
```

원-핫 인코딩의 변형으로 **원-핫 해싱(one-hot-hashging)** 기법이 있습니다.
이 방법은 어휘 사전에 있는 고유 토큰 수가 너무 커서 모두 다루기 어려울 때 사용합니다. 이 방식은 단어의 명시적 인덱스가 필요 없기 때문에 메모리를 절약하고 온라인 방식으로 데이터를 인코딩할 수 있습니다. 하지만 ***해시 충돌*** 이 있을 수 있으니 잘 조절해야합니다.

``` python
# 코드 6-4 해싱 기법을 사용한 단어 수준의 원-핫 인코딩하기(간단한 예)

samples = ['The cat sat on the mat.', 'The dog ate my homework.']

# 단어를 크기가 1,000인 벡터로 저장합니다.
# 1,000개(또는 그이상)의 단어가 있다면 해싱 충돌이 늘어나고 인코딩의 정확도가 감소될 것입니다
dimensionality = 1000
max_length = 10

results = np.zeros((len(samples), max_length, dimensionality))
for i, sample in enumerate(samples):
    for j, word in list(enumerate(sample.split()))[:max_length]:
        # 단어를 해싱하여 0과 1,000 사이의 랜덤한 정수 인덱스로 변환합니다.
        index = abs(hash(word)) % dimensionality
        results[i, j, index] = 1.
```

### 6.1.2 단어 임베딩 사용하기

단어와 벡터를 연관 하는 강력하고 인기 있는 방법은 **단어 임베딩** 이라는 밀집 **단어 벡터(word vector)** 를 사용하는 것입니다. 원-핫 인코딩으로 만든 벡터는 희소하고 고차원입니다. 하지만 단어 임베딩은 저차원의 실수형 벡터입니다.

단어 임베딩은 데이터로부터 학습됩니다. 보통 256, 512, 1024차원의 단어 임베딩을 사용하고, 원-핫 인코딩은 20000차원 또는 그 이상의 벡터인 경우가 많습니다. 즉 단어 임베딩이 더 많은 정보를 적은 자원에 저장합니다. 단어 임베딩을 만드는 방법은 두 가지입니다.

- 관심 대상인 문제와 함께 단어 임베딩을 학습합니다. 이런 경우 랜덤한 단어 벡터로 시작해서 신경망의 가중치를 학습하는 것과 같은 방식으로 단어 벡터를 학습합니다.
- 풀려는 문제가 아니고 다른 머신 러닝 작업에서 미리 계산된 단어 임베딩을 로드합니다. 이를 **사전 훈련된 단어 임베딩(pretrained word embedding)** 이라고 합니다.

#### Embedding 층을 사용하여 단어 임베딩 학습하기

단어와 밀집 벡터를 연관 짓는 가장 간단한 방법은 랜덤하게 벡터를 선택하는 것입니다. 이 방식의 문제점은 임베딩 공간이 구조적이지 않다는 것입니다. 의미 관계가 유사하더라도 완전 다른 임베딩을 가집니다.
그렇기에 단어 사이에 있는 의미 관계를 반영하여 임베딩을 진행해야 합니다.


<figure class = "align-center" style = "width : 400px">
  <img src= "https://www.researchgate.net/profile/Venkatesh_Saligrama/publication/304163868/figure/fig1/AS:375256100425729@1466479434837/Comparison-of-gender-bias-of-profession-words-across-two-embeddings-word2vec-trained-on.png" width="400" alt>
  <figcaption> 구글링해서 가져온 예시 </figcaption>
</figure>


단어 임베딩은 언어를 기하학적 공간에 매핑하는 것입니다. 일반적으로 두 단어 벡터 사이의 거리(L2 거리)는 이 단어 사이의 의미 거리와 관계되어 있습니다. 거리 외에 임베딩 공간의 특정 방향도 의미를 가질 수 있습니다.

의미있는 기하학적 변환의 예시는 성별, 복수(plural)과 같은 벡터가 있습니다. ('king' + 'female' => 'queen') 단어 임베딩 공간은 전형적으로 이런 해석 가능하고 잠재적으로 유용한 수천 개의 벡터를 특성으로 가집니다.

하지만 사람의 언어를 완벽하게 매핑해서 이상적인 단어 임베딩 공간을 만들기는 어렵습니다. 언어끼리도 종류가 많고 언어는 특정 문화와 환경을 반영하기 때문에 서로 동일하지 않습니다. 그렇기에 각 언어와 상황에 따라 임베딩 공간을 학습하는 것이 타당합니다.

이를 역전파 + 케라스를 이용해서 `Embedding`층을 학습할 수 있습니다.

``` python
# 코드 6-5 Embedding층의 객체 생성하기

from keras.layers import Embedding

# Embedding 층은 적어도 두 개의 매개변수를 받습니다.
# 가능한 토큰의 개수(여기서는 1,000으로 단어 인덱스 최댓값 + 1입니다)와 임베딩 차원(여기서는 64)입니다
# 인덱스는 0을 사용하지 않으므로 단어 인덱스는 1~999사이의 정수입니다
embedding_layer = Embedding(1000, 64)
```

`Embedding`층을 정수 인덱스를 밀집 벡터로 매핑하는 딕셔너리로 이해하는 것이 좋습니다. 정수를 입력으로 받아 내부 딕셔너리에서 이 정수에 연관된 벡터를 찾아 반환합니다. 딕셔너리 탐색은 효율적으로 수행됩니다. (텐서플로 백엔드에서는 tf.nn.embedding_lookup()함수를 사용하여 병렬 처리)

``` mermaid
graph LR;
  a(단어 인덱스) --> b(Embdding 층)
  b --> c(연관된 단어 벡터) 
```

#### 사전 훈련된 단어 임베딩 사용하기




### 6.1.3 모든 내용을 적용하기: 원본 텍스트에서 단어 임베딩까지

### 6.1.4 정리


## 6.2 순환 신경망 이해하기

### 6.2.1 케라스의 순환 층

### 6.2.2 LSTM과 GRU 층 이해하기

### 6.2.3 케라스를 사용한 LSTM 예제

### 6.2.4 정리

## 6.3 순환 신경망의 고급 사용법

### 6.3.1 기온 예측 문제

### 6.3.2 데이터 준비

### 6.3.3 상식 수준의 기준점

### 6.3.4 기본적인 머신 러닝 방법

### 6.3.5 첫 번째 순환 신경망

### 6.3.6 과대적합을 감소하기 위해 순환 드롭아웃 사용하기

### 6.3.7 스태킹 순환 층

### 6.3.8 양방향 RNN 사용하기

### 6.3.9 더 나아가서

### 6.3.10 정리

## 6.4 컨브넷을 사용한 시퀀스 처리

### 6.4.1 시퀀스 데이터를 위한 1D 합성곱 이해하기

### 6.4.2 시퀀스 데이터를 위한 1D 풀링

### 6.4.3 1D 컨브넷 구현

### 6.4.4 CNN과 RNN을 연결하여 긴 시퀀스 처리하기

### 6.4.5 정리

## 6.5 요약
